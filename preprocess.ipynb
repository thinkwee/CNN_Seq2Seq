{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:28:12.943893Z",
     "start_time": "2019-04-24T11:28:12.536159Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import spacy\n",
    "import _pickle as pickle\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集、测试集、验证集目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:40:04.584092Z",
     "start_time": "2019-04-24T11:40:04.579606Z"
    }
   },
   "outputs": [],
   "source": [
    "path_src = [\"test.txt.src\", \"train.txt.src\", \"val.txt.src\"]\n",
    "path_tgt = [\n",
    "    \"test.txt.tgt.tagged\", \"train.txt.tgt.tagged\", \"val.txt.tgt.tagged\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去标点噪音,转小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T01:57:37.305732Z",
     "start_time": "2019-04-20T01:57:37.300129Z"
    }
   },
   "outputs": [],
   "source": [
    "def depunc(path):\n",
    "    fr = codecs.open(path, encoding='utf-8')\n",
    "    fw = codecs.open(path + '.clean', 'w', encoding='utf-8')\n",
    "    for line in tqdm(fr):\n",
    "        if line != \"\":\n",
    "            line = line.lower()\n",
    "            line = re.sub(r\"--|-lrb-.*?-rrb- |'' |\\\"|`` |:|<t> |</t> |</t>\",\n",
    "                          \"\", line)\n",
    "            line = re.sub(r\"\\s\\.\\s\", r\" . \", line)\n",
    "            line = re.sub(r\"\\s\\?\\s\", r\" ? \", line)\n",
    "            line = re.sub(r\"\\s\\!\\s\", r\" ! \", line)\n",
    "            fw.write(line)\n",
    "            if ord(line[-1]) != 10:\n",
    "                fw.write(\"\\n\")\n",
    "    fr.close()\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得所有训练语料，用于训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T01:57:38.169664Z",
     "start_time": "2019-04-20T01:57:38.164255Z"
    }
   },
   "outputs": [],
   "source": [
    "def bow(path):\n",
    "    fr = codecs.open(path, encoding='utf-8')\n",
    "    fw = codecs.open('./data/corpus_total.txt', 'a', encoding='utf-8')\n",
    "    for line in tqdm(fr):\n",
    "        if line != \"\":\n",
    "            fw.write(line)\n",
    "    fr.close()\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用depunc清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T01:57:38.889365Z",
     "start_time": "2019-04-20T01:57:38.884174Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_model():\n",
    "    for src in path_src:\n",
    "        depunc(\"./data/\" + src)\n",
    "    for tgt in path_tgt:\n",
    "        depunc(\"./data/\" + tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T01:58:34.442325Z",
     "start_time": "2019-04-20T01:57:40.014927Z"
    }
   },
   "outputs": [],
   "source": [
    "prepare_for_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为fasttext训练词向量准备语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T01:59:17.485896Z",
     "start_time": "2019-04-20T01:59:17.481814Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_fasttext():\n",
    "    bow(\"./data/train.txt.src.clean\")\n",
    "    bow(\"./data/train.txt.tgt.tagged.clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T01:59:26.556980Z",
     "start_time": "2019-04-20T01:59:18.224695Z"
    }
   },
   "outputs": [],
   "source": [
    "prepare_for_fasttext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:28:07.437184Z",
     "start_time": "2019-04-24T11:28:07.434849Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dic():\n",
    "    dic = defaultdict(int)\n",
    "    with open(\"./data/corpus_total.txt\", \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            for word in line.split():\n",
    "                dic[word] += 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:28:49.829423Z",
     "start_time": "2019-04-24T11:28:17.870775Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = build_dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:31:53.480037Z",
     "start_time": "2019-04-24T11:31:53.477705Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:31:55.109652Z",
     "start_time": "2019-04-24T11:31:54.997845Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_dic = sorted(\n",
    "    dictionary.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立映射表并保存\n",
    "- 原文词典大小为40000\n",
    "- 文摘词典大小为10000\n",
    "- 0:PAD\n",
    "- 1:EOS\n",
    "- 2:UNK\n",
    "- 3:STR\n",
    "- source_input : W W W W UNK W W EOS PAD PAD PAD\n",
    "- target_input : STR w w w UNK w EOS PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:38:45.629795Z",
     "start_time": "2019-04-24T11:38:45.628147Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_size_src = 40000\n",
    "dict_size_tgt = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:38:45.970310Z",
     "start_time": "2019-04-24T11:38:45.920946Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 4\n",
    "word2id_src = dict()\n",
    "id2word_src = dict()\n",
    "word2id_src['PAD'] = 0\n",
    "word2id_src['EOS'] = 1\n",
    "word2id_src['UNK'] = 2\n",
    "word2id_src['SOS'] = 3\n",
    "id2word_src[0] = 'PAD'\n",
    "id2word_src[1] = 'EOS'\n",
    "id2word_src[2] = 'UNK'\n",
    "id2word_src[3] = 'SOS'\n",
    "for (k, v) in sorted_dic:\n",
    "    word2id_src[k] = count\n",
    "    id2word_src[count] = k\n",
    "    count += 1\n",
    "    if count == dict_size_src:\n",
    "        break\n",
    "pickle.dump(word2id_src, open(\"./data/word2id_src.dat\", \"wb\"), True)\n",
    "pickle.dump(id2word_src, open(\"./data/id2word_src.dat\", \"wb\"), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:38:46.595588Z",
     "start_time": "2019-04-24T11:38:46.586826Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 4\n",
    "word2id_tgt = dict()\n",
    "id2word_tgt = dict()\n",
    "word2id_tgt['PAD'] = 0\n",
    "word2id_tgt['EOS'] = 1\n",
    "word2id_tgt['UNK'] = 2\n",
    "word2id_tgt['SOS'] = 3\n",
    "id2word_tgt[0] = 'PAD'\n",
    "id2word_tgt[1] = 'EOS'\n",
    "id2word_tgt[2] = 'UNK'\n",
    "id2word_tgt[3] = 'SOS'\n",
    "for (k, v) in sorted_dic:\n",
    "    word2id_tgt[k] = count\n",
    "    id2word_tgt[count] = k\n",
    "    count += 1\n",
    "    if count == dict_size_tgt:\n",
    "        break\n",
    "pickle.dump(word2id_tgt, open(\"./data/word2id_tgt.dat\", \"wb\"), True)\n",
    "pickle.dump(id2word_tgt, open(\"./data/id2word_tgt.dat\", \"wb\"), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理待训练语料\n",
    "- 原文限制长度为400以内，标题限制在55以内\n",
    "- 替换为one-hot下标\n",
    "- 补上PAD、EOS、UNK、STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:38:48.122176Z",
     "start_time": "2019-04-24T11:38:48.120598Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC_LENGTH = 500\n",
    "TGT_LENGTH = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:38:54.912058Z",
     "start_time": "2019-04-24T11:38:54.908009Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_to_one_hot(path, output, word2id):\n",
    "    if \"tgt\" in path:\n",
    "        restrict_len = TGT_LENGTH\n",
    "    else:\n",
    "        restrict_len = SRC_LENGTH\n",
    "    one_hot_matrix = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            one_hot_list = [\n",
    "                word2id[word] if word in word2id else 2\n",
    "                for word in line.split(' ')[:-1]\n",
    "            ]\n",
    "            if \"tgt\" in path and output:\n",
    "                one_hot_list.insert(0, 3)\n",
    "            one_hot_list = one_hot_list[:restrict_len - 1]\n",
    "            one_hot_list.append(1)\n",
    "            if len(one_hot_list) < restrict_len:\n",
    "                for _ in range(restrict_len - len(one_hot_list)):\n",
    "                    one_hot_list.append(0)\n",
    "            one_hot_matrix.append(one_hot_list)\n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tgt处理两次，\n",
    "- .onehot后缀是加了开始符号，整体右移一个单位。作为decoder输入\n",
    "- .gold后缀是原始语料，作为gold output计算损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:39:57.759652Z",
     "start_time": "2019-04-24T11:39:57.755054Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_for_model():\n",
    "    for src in path_src:\n",
    "        matrix = word_to_one_hot(\n",
    "            \"./data/\" + src + \".clean\", output=False, word2id=word2id_src)\n",
    "        matrix = np.asarray(matrix)\n",
    "        pickle.dump(\n",
    "            torch.from_numpy(matrix), open(\"./data/\" + src + \".onehot\", \"wb\"),\n",
    "            True)\n",
    "    for tgt in path_tgt:\n",
    "        matrix = word_to_one_hot(\n",
    "            \"./data/\" + tgt + \".clean\", output=False, word2id=word2id_tgt)\n",
    "        matrix = np.asarray(matrix)\n",
    "\n",
    "        pickle.dump(\n",
    "            torch.from_numpy(matrix), open(\"./data/\" + tgt + \".gold\", \"wb\"),\n",
    "            True)\n",
    "    for tgt in path_tgt:\n",
    "        matrix = word_to_one_hot(\n",
    "            \"./data/\" + tgt + \".clean\", output=True, word2id=word2id_tgt)\n",
    "        matrix = np.asarray(matrix)\n",
    "\n",
    "        pickle.dump(\n",
    "            torch.from_numpy(matrix), open(\"./data/\" + tgt + \".onehot\", \"wb\"),\n",
    "            True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:41:06.422735Z",
     "start_time": "2019-04-24T11:40:15.556765Z"
    }
   },
   "outputs": [],
   "source": [
    "one_hot_for_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:43:05.360761Z",
     "start_time": "2019-04-24T11:43:05.352488Z"
    }
   },
   "outputs": [],
   "source": [
    "matrix = pickle.load(open(\"./data/test.txt.tgt.tagged.onehot\", \"rb\"))\n",
    "print(len(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:43:06.297856Z",
     "start_time": "2019-04-24T11:43:06.295278Z"
    }
   },
   "outputs": [],
   "source": [
    "print(matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:43:10.505780Z",
     "start_time": "2019-04-24T11:43:10.503332Z"
    }
   },
   "outputs": [],
   "source": [
    "s = [id2word[id] for id in matrix[0].numpy()]\n",
    "print(' '.join(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计文摘的长度作为mask传给模型\n",
    "- 方便计算maskedNLLLoss\n",
    "- 是截断后文摘的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:43:15.689710Z",
     "start_time": "2019-04-24T11:43:15.687020Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_len_mask(path):\n",
    "    len_mask = []\n",
    "    onehot = pickle.load(open(path, \"rb\"))\n",
    "    for sentence_onehot in tqdm(onehot):\n",
    "        count = 0\n",
    "        for i in sentence_onehot:\n",
    "            count += 1\n",
    "            if i == 1:\n",
    "                break\n",
    "        len_mask.append(count)\n",
    "    len_mask = torch.from_numpy(np.asarray(len_mask))\n",
    "    pickle.dump(len_mask, open(\"./data/\" + tgt + \".mask\", \"wb\"), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:43:48.596831Z",
     "start_time": "2019-04-24T11:43:16.583169Z"
    }
   },
   "outputs": [],
   "source": [
    "for tgt in path_tgt:\n",
    "    make_len_mask(\"./data/\" + tgt + \".onehot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:44:55.144375Z",
     "start_time": "2019-04-24T11:44:55.137911Z"
    }
   },
   "outputs": [],
   "source": [
    "mask_test = pickle.load(open(\"./data/test.txt.tgt.tagged.mask\", \"rb\"))\n",
    "print(mask_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:45:07.011559Z",
     "start_time": "2019-04-24T11:45:06.869423Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tgt = pickle.load(open(\"./data/train.txt.tgt.tagged.onehot\", \"rb\"))\n",
    "vali_tgt = pickle.load(open(\"./data/val.txt.tgt.tagged.onehot\", \"rb\"))\n",
    "test_tgt = pickle.load(open(\"./data/test.txt.tgt.tagged.onehot\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:45:07.282906Z",
     "start_time": "2019-04-24T11:45:07.275874Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:45:07.676195Z",
     "start_time": "2019-04-24T11:45:07.671214Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(vali_tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T11:45:08.179191Z",
     "start_time": "2019-04-24T11:45:08.171170Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(test_tgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从onehot中恢复出文本\n",
    "- 即获得截断长度的文本，另存为一份语料，用于fairseq训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:32:47.702085Z",
     "start_time": "2019-04-20T08:32:47.698573Z"
    }
   },
   "outputs": [],
   "source": [
    "parts = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:00:43.421773Z",
     "start_time": "2019-04-20T08:00:16.414752Z"
    }
   },
   "outputs": [],
   "source": [
    "for p in parts:\n",
    "    with open(\"./data/fairseq/\" + p + \".src\", \"w\") as f:\n",
    "        onehot = pickle.load(open(\"./data/\" + p + \".txt.src.onehot\", \"rb\"))\n",
    "        for sentence in tqdm(onehot):\n",
    "            s = [id2word[id] for id in sentence.numpy()]\n",
    "            f.write(' '.join(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:00:49.552762Z",
     "start_time": "2019-04-20T08:00:45.278451Z"
    }
   },
   "outputs": [],
   "source": [
    "for p in parts:\n",
    "    with open(\"./data/fairseq/\" + p + \".tgt\", \"w\") as f:\n",
    "        onehot = pickle.load(\n",
    "            open(\"./data/\" + p + \".txt.tgt.tagged.onehot\", \"rb\"))\n",
    "        for sentence in tqdm(onehot):\n",
    "            s = [id2word[id] for id in sentence.numpy()]\n",
    "            f.write(' '.join(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接将clean语料截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:42:50.891696Z",
     "start_time": "2019-04-20T08:42:40.788147Z"
    }
   },
   "outputs": [],
   "source": [
    "for p in parts:\n",
    "    with open(\"./data/fairseq/\" + p + \".src\", \"w\") as fw:\n",
    "        fr = open(\"./data/\" + p + \".txt.src.clean\", \"r\")\n",
    "        for sentence in tqdm(fr):\n",
    "            s = ' '.join(sentence.split(' ')[:SRC_LENGTH])\n",
    "            fw.write(s)\n",
    "            if ord(s[-1]) != 10:\n",
    "                fw.write(\"\\n\")\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:44:30.590064Z",
     "start_time": "2019-04-20T08:44:29.454149Z"
    }
   },
   "outputs": [],
   "source": [
    "for p in parts:\n",
    "    with open(\"./data/fairseq/\" + p + \".tgt\", \"w\") as fw:\n",
    "        fr = open(\"./data/\" + p + \".txt.tgt.tagged.clean\", \"r\")\n",
    "        for sentence in tqdm(fr):\n",
    "            s = ' '.join(sentence.split(' ')[:TGT_LENGTH])\n",
    "            fw.write(s)\n",
    "            if ord(s[-1]) != 10:\n",
    "                fw.write(\"\\n\")\n",
    "        fr.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "531px",
    "width": "308px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
